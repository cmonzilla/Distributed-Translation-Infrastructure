Parameter tuning with the Oister framework is done based on the following three files:

1. <souirce-file-name>.lattices        - stores the translation tree/graph per translated sentence with the information on the transitions between partial hypothesis and source and target phrases and added costs
2. <souirce-file-name>.feature_scores  - stores, per sentence, information on which partial hypothesis added/used which feature weights (multiplied with lambdas?) in which partial hypothesis score computation
3. oister.conf.feature_id2name         - this one maps the oister feature name to the id number used to identify the feature within the lattice feature score file

Let us further specify the format/give an example of the files mentioned above:

QUESTIONS:
 1. What shall we do in case that there is no translation for a sentence? 
    What shall we dump?: Nothing or throw an exception? - shall not happen any ways!
 2. The scores file per sentence stores all the nodes with the feature values, do
    the nodes have to be ordered by the node id?
    (we could actually do the ordering later on with the standard sort utility)

*********************************
<source-file-name>.lattices
*********************************

The first node for the sentence is the end dummy node for which there are no target translation phrases - empty and the score deltas are all zero

<SENT ID=XXXXXXX>
TO_NODE_ID_1   FROM_NODE_ID_1|||TARGET_PHRASE|||SCORE_DELTA FROM_NODE_ID_2|||TARGET_PHRASE|||SCORE_DELTA ... FROM_NODE_ID_K|||TARGET_PHRASE|||SCORE_DELTA
...
<COVERVECS>TO_NODE_ID_1-FROM_NODE_ID_1:BEGIN_SROURCE_PHRASE_IDX:END_SROURCE_PHRASE_IDX TO_NODE_ID_1-FROM_NODE_ID_2:BEGIN_SROURCE_PHRASE_IDX:END_SROURCE_PHRASE_IDX …</COVERVECS>
</SENT>
...

The first present TO_NODE_ID_1 has the maximum id value and is a dummy end super
state connected to all the states of the last actual stack level. The first stack
state shall have the smallest state id 0. The lattice dumping happens backwards:
from the end super state to the front of the stack.

Here XXXXXXX is an unsigned integer, starting with 0, representing the sentence id.
Each TO_NODE_ID_YYYYYY entry is placed on a separate line
TO_NODE_ID and FROM_NODE_ID_1 have 3 spaces in between

The multiple FROM_NODE_ID_XXX for the same TO_NODE_ID_YYY mean that the TO_NODE_ID_YYY has a number of states recombined into it.
So those additional FROM_NODE_ID_XXX nodes are from those recombined states.

Every other incoming edge, e.g. FROM_NODE_ID_2, is divided from the previous score delta with a space.
If there is no target phrase translation, for the end dummy one referencing all the other ones, then TARGET_PHRASE turns into an empty string.
If there is no known translation for a source phrase then the corresponding source phrase is used instead of the target phrase.
<COVERVECS> contains TO-FROM node id pairs with the corresponding index interval for the last translated source phrase
The <COVERVECS> tag seems to store everything in one line
There is one <COVERVECS></COVERVECS> entry per sentence 
There are multiple <SENT></SENT> entries per file.
NODE_IDs seem to start from 1, the 0 id is for the first hypothesis.

*********************************
<source-file-name>.feature_scores
*********************************

<SENT ID=0>
NODE_ID_1 FEATURE_ID_A11=FEATURE_WEIGHT ... FEATURE_ID_A1N1=FEATURE_WEIGHT
...
NODE_ID_Z FEATURE_ID_AZ1=FEATURE_WEIGHT ... FEATURE_ID_AZNZ=FEATURE_WEIGHT
</SENT>

Here the nod ids and feature weights are space separated.
Nodes are have an ascending order in their ids.
ONLY the feature weights added to the node partial/total cost are listed in the file
FEATURE_IDs start from zero

Features with zero weights do not have to be in the node line even if they are used.

Questions to be answered:

1. Do we take into account the features used in the future cost computation or just the features from the current score computation?
   - No, the future costs are not taken into account.

2. Do the feature values have to be multiplied with lambda’s or those shall be just the original weights?
   - These are to be the original weights.

3. Is the Language model weight is the joint probability of the new phrase with the given history, right?
   - Yes, it is so.

4. The word penalty is the #words * the word_penalty?
   - No, this is only the #words (can be positive or negative)

5. The distortion penalty is the linear distortion cost and not just the value of lin_dist_penalty?
   - No, this is only the distortion cost without lambda

6. The lex_dm_fe values are the distortion model features but what are lex_hrm_fe?
   - lex_hrm_fe is a hierarchical reordering model, which is not present in my decoder right now.

7. The phrase_table[4] (phrase_penalty) is just a constant value, and is to be used for any hypothesis as is.
   - Yes, grep -v " 4=" <file_name>.feature_scores | grep -v "SENT" gives an empty result, so each entry had
     a phrase_table[4] used.

*********************************
oister.conf.feature_id2name
*********************************

This file just matches the feature names of the oister.conf file to the
feature id numbers, as used in a <source-file-name>.feature_scores file. 
The list of features should be easily extendable to more models.
Consider the following example:

0       phrase_table[0]
1       phrase_table[1]
2       phrase_table[2]
3       phrase_table[3]
4       phrase_table[4]
5       lm_weight
6       word_penalty
7       distortion_cost
8       lex_dm_fe[0]
9       lex_dm_fe[1]
10      lex_dm_fe[2]
11      lex_dm_fe[3]
12      lex_dm_fe[4]
13      lex_dm_fe[5]
14      lex_dm_fe[6]
15      lex_dm_fe[7]
16      lex_hrm_fe[0]
17      lex_hrm_fe[1]
18      lex_hrm_fe[2]
19      lex_hrm_fe[3]
20      lex_hrm_fe[4]
21      lex_hrm_fe[5]
22      lex_hrm_fe[6]
23      lex_hrm_fe[7]

In the example above the lex_hrm_fe are the features that are not present inside the current basic translation system.
Also the lex_dm_fe[3] and lex_dm_fe[7] are not present either. In out system phrase_table[4] is the phrase penalty.
