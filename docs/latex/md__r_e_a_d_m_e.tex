\#$\ast$$\ast$\+The Basic Phrase-\/\+Based Statistical Machine Translation Tool$\ast$$\ast$

{\bfseries Author\+:} \href{https://nl.linkedin.com/in/zapreevis}{\tt Dr. Ivan S. Zapreev}

{\bfseries Project pages\+:} \href{https://github.com/ivan-zapreev/Back-Off-Language-Model-SMT}{\tt Git-\/\+Hub-\/\+Project}

\subsection*{Introduction}

This is a fork project from the Back Off Language Model(s) for S\+M\+T project aimed at creating the entire phrase-\/based S\+M\+T translation infrastructure. This project follows a client/server atchitecture based on Web\+Sockets for C++ and consists of the three main applications\+:


\begin{DoxyItemize}
\item {\bfseries bpbd-\/client} -\/ is a thin client to send the translation job requests to the translation server and obtain results
\item {\bfseries bpbd-\/server} -\/ the the translation server consisting of the following main components\+:
\begin{DoxyItemize}
\item {\itshape Decoder} -\/ the decoder component responsible for translating text from one language into another
\item {\itshape L\+M} -\/ the language model implementation allowing for seven different trie implementations and responsible for estimating the target language phrase probabilities.
\item {\itshape T\+M} -\/ the translation model implementation required for providing source to target language phrase translation and the probailities thereof.
\item {\itshape R\+M} -\/ the reordering model implementation required for providing the possible translation order changes and the probabilities thereof
\end{DoxyItemize}
\item {\bfseries lm-\/query} -\/ a stand-\/alone language model query tool that allows to perform labguage model queries and estimate the joint phrase probabilities.
\end{DoxyItemize}

To keep a clear view of the used terminology further we will privide some details on the phrase based statistical machine translation as given on the picture below.



The entire phrase-\/based statistical machine translation is based on learned statistical correlations between words and phrases of an example translation text, also called parallel corpus or corpora. Clearly, if the training corpora is large enough then it allows to cover most source/target language words and phrases and shall have enough information for approximating a translation of an arbitrary text. However, before this information can be extracted, the parallel corpora undergoes the process called {\itshape word alignment} which is aimed at estimating which words/phrases in the source language correspond to which words/phrases in the target language. As a result, we obtain two statistical models\+:


\begin{DoxyEnumerate}
\item The Translation model -\/ providing phrases in the source language with learned possible target language translations and the probabilities thereof.
\item The Reordering model -\/ storing information about probable translation orders of the phrases within the source text, based on the observed source and target phrases and alignment thereof.
\end{DoxyEnumerate}

The last model, possibly learned from a different corpus in a target language, is the Language model. Its purpose is to reflect the likelihood of this or that phrase in the target language to occur. In other words it is used to evaluate the obtained translation for being {\itshape sound} in the target language.

With these three models at hand one can perform decoding, which is a synonim to a translation process. S\+M\+T decoding is performed by exploring the state space of all possible translations and reorderings of the source language phrases within one sentence and then looking for the most probable translations, as indicated at the bottom part of the picture above.

The rest of the document is organized as follows\+:


\begin{DoxyEnumerate}
\item \href{#project-structure}{\tt Project structure} -\/ Gives the file and folder structure of the project
\item \href{#supported-platforms}{\tt Supported platforms} -\/ Indicates the project supported platforms
\item \href{#building-the-project}{\tt Building the project} -\/ Describes the process of building the project
\item \href{#using-software}{\tt Using software} -\/ Explain how the software is to be used
\item \href{#input-file-formats}{\tt Input file formats} -\/ Provides examples of the input file formats
\item \href{#code-documentation}{\tt Code documentation} -\/ Refers to the project documentation
\item \href{#external-libraries}{\tt External libraries} -\/ Lists the included external libraries
\item \href{#general-design}{\tt General design} -\/ Outlines the general software desing
\item \href{#software-details}{\tt Software details} -\/ Goes about some of the software details
\item \href{#literature-and-references}{\tt Literature and references} -\/ Presents the list of used literature
\item \href{#licensing}{\tt Licensing} -\/ States the licensing strategy of the project
\item \href{#history}{\tt History} -\/ Stores a short history of this document
\end{DoxyEnumerate}

\subsection*{Project structure}

This is a Netbeans 8.\+0.\+2 project, based on cmake, and its top-\/level structure is as follows\+:


\begin{DoxyItemize}
\item $\ast$$\ast${\ttfamily \mbox{[}Project-\/\+Folder\mbox{]}}$\ast$$\ast$/
\begin{DoxyItemize}
\item {\bfseries doc/} -\/ contains the project-\/related documents including the Doxygen-\/generated code documentation and images
\item {\bfseries ext/} -\/ stores the external header only libraries used in the project
\item {\bfseries inc/} -\/ stores the C++ header files of the implementation
\item {\bfseries src/} -\/ stores the C++ source files of the implementation
\item {\bfseries nbproject/} -\/ stores the Netbeans project data, such as makefiles
\item {\bfseries data/} -\/ stores the test-\/related data such as test models and query intput files, as well as some experimental results.
\item L\+I\+C\+E\+N\+S\+E -\/ the code license (G\+P\+L 2.\+0)
\item C\+Make\+Lists.\+txt -\/ the cmake build script for generating the project\textquotesingle{}s make files
\item \hyperlink{_r_e_a_d_m_e_8md}{R\+E\+A\+D\+M\+E.\+md} -\/ this document
\item Doxyfile -\/ the Doxygen configuration file
\end{DoxyItemize}
\end{DoxyItemize}

\subsection*{Supported platforms}

This project supports two major platforms\+: Linux and Mac Os X. It has been successfully build and tested on\+:


\begin{DoxyItemize}
\item {\bfseries Centos 6.\+6 64-\/bit} -\/ Complete functionality.
\item {\bfseries Ubuntu 15.\+04 64-\/bit} -\/ Complete functionality.
\item {\bfseries Mac O\+S X Yosemite 10.\+10 64-\/bit} -\/ Limited by inability to collect memory-\/usage statistics.
\end{DoxyItemize}

{\bfseries Notes\+:}


\begin{DoxyEnumerate}
\item There was only a limited testing performed on 32-\/bit systems.
\item The project must be possible to build on Windows platform under \href{https://www.cygwin.com/}{\tt Cygwin}.
\end{DoxyEnumerate}

\subsection*{Building the project}

Building this project requires {\bfseries gcc} version $>$= {\itshape 4.\+9.\+1} and {\bfseries cmake} version $>$= 2.\+8.\+12.\+2. The project can be build in two ways\+:


\begin{DoxyItemize}
\item From the Netbeans environment by running Build in the I\+D\+E
\begin{DoxyItemize}
\item Perform {\ttfamily mkdir build} in the project folder.
\item In Netbeans menu\+: {\itshape Tools/\+Options/\char`\"{}\+C/\+C++\char`\"{}} make sure that the cmake executable is properly set.
\item Netbeans will always run cmake for the D\+E\+B\+U\+G version of the project
\item To build project in R\+E\+L\+E\+A\+S\+E version use building from Linux console
\end{DoxyItemize}
\item From the Linux command-\/line console perform the following steps
\begin{DoxyItemize}
\item {\ttfamily cd \mbox{[}Project-\/\+Folder\mbox{]}}
\item {\ttfamily mkdir build}
\item {\ttfamily cd build}
\item {\ttfamily cmake -\/\+D\+C\+M\+A\+K\+E\+\_\+\+B\+U\+I\+L\+D\+\_\+\+T\+Y\+P\+E=Release ..} O\+R {\ttfamily cmake -\/\+D\+C\+M\+A\+K\+E\+\_\+\+B\+U\+I\+L\+D\+\_\+\+T\+Y\+P\+E=Debug ..}
\item {\ttfamily make -\/j \mbox{[}N\+U\+M\+B\+E\+R-\/\+O\+F-\/\+T\+H\+R\+E\+A\+D\+S\mbox{]}} add {\ttfamily V\+E\+R\+B\+O\+S\+E=1} to make the compile-\/time options visible
\end{DoxyItemize}
\end{DoxyItemize}

The binaries will be generated and placed into $\ast$./build/$\ast$ folder. In order to clean the project from the command line run {\ttfamily make clean}. Cleaning from Netbeans is as simple calling the {\ttfamily Clean and Build} from the {\ttfamily Run} menu.

\subsubsection*{Project compile-\/time parameters}

There is a number of project parameters that at this moment are to be chosen only once before the project is compiled. These are otherwise called the compile-\/time parameters. Further we consider the most important of them and indicate where all of them are to be found.

{\bfseries Loggin level\+:} Logging is important when debugging software or providing an additional used information during the program\textquotesingle{}s runtime. Yet additional output actions come at a prise and can negatively influence the program\textquotesingle{}s performance. This is why it is important to be able to disable certain logging levels within the program not only during its runtime but also at compile time. The possible range of project\textquotesingle{}s logging levels, listed incrementally is\+: E\+R\+R\+O\+R, W\+A\+R\+N\+I\+N\+G, U\+S\+A\+G\+E, R\+E\+S\+U\+L\+T, I\+N\+F\+O, I\+N\+F\+O1, I\+N\+F\+O2, I\+N\+F\+O3, D\+E\+B\+U\+G, D\+E\+B\+U\+G1, D\+E\+B\+U\+G2, D\+E\+B\+U\+G3, D\+E\+B\+U\+G4. One can limit the logging level range available at runtime by setting the {\ttfamily L\+O\+G\+E\+R\+\_\+\+M\+\_\+\+G\+R\+A\+M\+\_\+\+L\+E\+V\+E\+L\+\_\+\+M\+A\+X} constaint value in the {\ttfamily ./inc/common/utils/logging/logger.hpp} header file.

{\bfseries Sanity checks\+:} When program is not running as expected, it could be caused by the internal software errors that are not detectable runtime. It is therefore possible to enable/disable software internal sanity checks by setting the {\ttfamily D\+O\+\_\+\+S\+A\+N\+I\+T\+Y\+\_\+\+C\+H\+E\+C\+K\+S} constand in the {\ttfamily ./inc/common/utils/exceptions.hpp} header file. Note that enabling the sanity checks does not guarantee that the internal error will be found and will have a negative effect on the program\textquotesingle{}s performance. Yet, it might help to identify errors with e.\+g. input file formats and alike.

{\bfseries Server configs\+:} There is a number of translation server common parameters used in decoding, translation, reordering anb language models. Those are to be found in the {\ttfamily ./inc/server/server\+\_\+configs.hpp}. Please be carefull changing them\+:


\begin{DoxyItemize}
\item {\ttfamily U\+N\+K\+N\+O\+W\+N\+\_\+\+L\+O\+G\+\_\+\+P\+R\+O\+B\+\_\+\+W\+E\+I\+G\+H\+T} -\/ The value used for the unknown probability weight \+\_\+(log10 scale)\+\_\+
\item {\ttfamily Z\+E\+R\+O\+\_\+\+L\+O\+G\+\_\+\+P\+R\+O\+B\+\_\+\+W\+E\+I\+G\+H\+T} -\/ The value used for the \textquotesingle{}zero\textquotesingle{} probability weight \+\_\+(log10 scale)\+\_\+
\item {\ttfamily tm\+::\+N\+U\+M\+\_\+\+T\+M\+\_\+\+F\+E\+A\+T\+U\+R\+E\+S} -\/ The number of the translation model features, defines the number of features read per entry in from the translation model input file.
\item {\ttfamily tm\+::\+T\+M\+\_\+\+M\+A\+X\+\_\+\+T\+A\+R\+G\+E\+T\+\_\+\+P\+H\+R\+A\+S\+E\+\_\+\+L\+E\+N} -\/ The maximum length of the target phrase to be considered, this defines the maximum number of tokens to be stored per translation entry
\item {\ttfamily lm\+::\+N\+U\+M\+\_\+\+L\+M\+\_\+\+F\+E\+A\+T\+U\+R\+E\+S} -\/ The number of languahe model features, the program currenly supports only one value\+: {\ttfamily 1}
\item {\ttfamily lm\+::\+L\+M\+\_\+\+M\+\_\+\+G\+R\+A\+M\+\_\+\+L\+E\+V\+E\+L\+\_\+\+M\+A\+X} -\/ The languahe model maximum level, the maximum number of words in the language model phrase
\item {\ttfamily lm\+::\+L\+M\+\_\+\+H\+I\+S\+T\+O\+R\+Y\+\_\+\+L\+E\+N\+\_\+\+M\+A\+X} -\/ {\bfseries do not change} this parameter
\item {\ttfamily lm\+::\+L\+M\+\_\+\+M\+A\+X\+\_\+\+Q\+U\+E\+R\+Y\+\_\+\+L\+E\+N} -\/ {\bfseries do not change} this parameter
\item {\ttfamily lm\+::\+D\+E\+F\+\_\+\+U\+N\+K\+\_\+\+W\+O\+R\+D\+\_\+\+L\+O\+G\+\_\+\+P\+R\+O\+B\+\_\+\+W\+E\+I\+G\+H\+T} -\/ The default unknown word probability weight, for the case the {\ttfamily $<$unk$>$} entry is not present in the language model file \+\_\+(log10 scale)\+\_\+
\item {\ttfamily rm\+::\+N\+U\+M\+\_\+\+R\+M\+\_\+\+F\+E\+A\+T\+U\+R\+E\+S} -\/ The maximum number of reordering model features, the only two currently supported values are\+: {\ttfamily 6} and {\ttfamily 8}.
\end{DoxyItemize}

{\bfseries Decoder configs\+:} There is a number of decoder-\/specific parameters that can be configured runtime. These are located in {\ttfamily ./inc/server/decoder/de\+\_\+configs.hpp}, please be careful changing them\+:


\begin{DoxyItemize}
\item {\ttfamily M\+A\+X\+\_\+\+W\+O\+R\+D\+S\+\_\+\+P\+E\+R\+\_\+\+S\+E\+N\+T\+E\+N\+C\+E} -\/ The maximum allowed number of words/tokens per sentence to translate.
\end{DoxyItemize}

{\bfseries L\+M configs\+:} There is a number of Language-\/model-\/specific parameters that can be configured runtime. These are located in {\ttfamily ./inc/server/lm/lm\+\_\+configs.hpp}, please be careful changing them\+:


\begin{DoxyItemize}
\item {\ttfamily lm\+\_\+word\+\_\+index} -\/ the word index type to be used, the possible values are\+:
\begin{DoxyItemize}
\item {\ttfamily basic\+\_\+word\+\_\+index} -\/ the basic word index that just loads the uni-\/grams in the same order as in the L\+M model file and give them consequtive id values.
\item {\ttfamily counting\+\_\+word\+\_\+index} -\/ the basic word index that counts the number of times the unigram occurs in the L\+M model file and gives lower ids to the more frequent unigrams. This ensures some performance boost (within 10\%) in querying certain types of langue models but requires longer loading times.
\item {\ttfamily optimizing\+\_\+word\+\_\+index$<$basic\+\_\+word\+\_\+index$>$} -\/ the optimizing word index is based on the linear probing hash map so it is the fastest, it uses a basic word index as a bootstrap word index for issuing the ids.
\item {\ttfamily optimizing\+\_\+word\+\_\+index$<$counting\+\_\+word\+\_\+index$>$} -\/ the optimizing word index is based on the linear probing hash map so it is the fastest, it uses a counting word index as a bootstrap word index for issuing the ids.
\item {\ttfamily hashing\+\_\+word\+\_\+index} -\/ the hashing word index is a discontinuous word index that does not issue the unigram ids consequently but rather associates each unigram with its hash value, the latter is taken to be an id. This is the only type of index supported by the hash-\/based {\ttfamily h2d\+\_\+map\+\_\+trie}.
\end{DoxyItemize}
\item {\ttfamily lm\+\_\+model\+\_\+type} -\/ the model type to be used, the possible values (trie types) are, for performance comparison thereof see \href{#performance-evaluation}{\tt Performance Evaluation}\+:
\begin{DoxyItemize}
\item {\ttfamily c2d\+\_\+hybrid\+\_\+trie$<$lm\+\_\+word\+\_\+index$>$} -\/ contains the context-\/to-\/data mapping trie implementation based on {\ttfamily std\+::unordered} map and ordered arrays.
\item {\ttfamily c2d\+\_\+map\+\_\+trie$<$lm\+\_\+word\+\_\+index$>$} -\/ contains the context-\/to-\/data mapping trie implementation based on {\ttfamily std\+::unordered map}.
\item {\ttfamily c2w\+\_\+array\+\_\+trie$<$lm\+\_\+word\+\_\+index$>$} -\/ contains the context-\/to-\/ word mapping trie implementation based on ordered arrays.
\item {\ttfamily g2d\+\_\+map\+\_\+trie$<$lm\+\_\+word\+\_\+index$>$} -\/ contains the m-\/gram-\/to-\/data mapping trie implementation based on self-\/made hash maps.
\item {\ttfamily h2d\+\_\+map\+\_\+trie$<$lm\+\_\+word\+\_\+index$>$} -\/ contains the hash-\/to-\/data mapping trie based on the linear probing hash map imlementation.
\item {\ttfamily w2c\+\_\+array\+\_\+trie$<$lm\+\_\+word\+\_\+index$>$} -\/ contains the word-\/to-\/ context mapping trie implementation based on ordered arrays.
\item {\ttfamily w2c\+\_\+hybrid\+\_\+trie$<$lm\+\_\+word\+\_\+index$>$} -\/ contains the word-\/to-\/ context mapping trie implementation based on {\ttfamily std\+::unordered} map and ordered arrays.
\end{DoxyItemize}
\item {\ttfamily lm\+\_\+model\+\_\+reader} -\/ the model reader is basically the file reader type one can use to load the model, currently there are three model reader types available, with {\ttfamily cstyle\+\_\+file\+\_\+reader} being the default\+:
\begin{DoxyItemize}
\item {\ttfamily file\+\_\+stream\+\_\+reader} -\/ uses the C++ streams to read from files, the slowest
\item {\ttfamily cstyle\+\_\+file\+\_\+reader} -\/ uses C-\/style file reading functions, faster than {\ttfamily file\+\_\+stream\+\_\+reader}
\item {\ttfamily memory\+\_\+mapped\+\_\+file\+\_\+reader} -\/ uses memory-\/mapped files which, faster than {\ttfamily cstyle\+\_\+file\+\_\+reader}, consumes twise the file size memory (virtual R\+A\+M).
\end{DoxyItemize}
\item {\ttfamily lm\+\_\+builder\+\_\+type} -\/ currently there is just one builder type available\+: {\ttfamily lm\+\_\+basic\+\_\+builder$<$lm\+\_\+model\+\_\+reader$>$}.
\end{DoxyItemize}

Note that not all of the combinations of the {\ttfamily lm\+\_\+word\+\_\+index} and {\ttfamily lm\+\_\+model\+\_\+type} can work together, this is reported runtime after the program is build. Some additional details on the preferred configurations can be also found in the {\ttfamily ./inc/server/lm/lm\+\_\+consts.hpp} header file comments. The default and the most optimal performance/memory ratio configuration is {\ttfamily lm\+\_\+word\+\_\+index} being set to {\ttfamily hashing\+\_\+word\+\_\+index} and {\ttfamily lm\+\_\+model\+\_\+type} begin set to {\ttfamily h2d\+\_\+map\+\_\+trie$<$lm\+\_\+word\+\_\+index$>$}.

{\bfseries T\+M configs\+:} There is a number of Translation-\/model-\/specific parameters that can be configured runtime. These are located in {\ttfamily ./inc/server/tm/tm\+\_\+configs.hpp}, please be careful changing them\+:


\begin{DoxyItemize}
\item {\ttfamily tm\+\_\+model\+\_\+type} -\/ currently there is just one model type available\+: {\ttfamily tm\+\_\+basic\+\_\+model}.
\item {\ttfamily tm\+\_\+model\+\_\+reader} -\/ the same as {\ttfamily lm\+\_\+model\+\_\+reader} for \+\_\+\char`\"{}\+L\+M configs\char`\"{}\+\_\+ above.
\item {\ttfamily tm\+\_\+builder\+\_\+type} -\/ currently there is just one builder byte available\+: {\ttfamily tm\+\_\+basic\+\_\+builder$<$tm\+\_\+model\+\_\+reader$>$}.
\end{DoxyItemize}

{\bfseries R\+M configs\+:} There is a number of Reordering-\/model-\/specific parameters that can be configured runtime. These are located in {\ttfamily ./inc/server/rm/rm\+\_\+configs.hpp}, please be careful changing them\+:


\begin{DoxyItemize}
\item {\ttfamily rm\+\_\+model\+\_\+type} -\/ currently there is just one model type available\+: {\ttfamily rm\+\_\+basic\+\_\+model}.
\item {\ttfamily rm\+\_\+model\+\_\+reader} -\/ the same as {\ttfamily lm\+\_\+model\+\_\+reader} for \+\_\+\char`\"{}\+L\+M configs\char`\"{}\+\_\+ above.
\item {\ttfamily rm\+\_\+builder\+\_\+type} -\/ currently there is just one builder byte available\+: {\ttfamily rm\+\_\+basic\+\_\+builder$<$rm\+\_\+model\+\_\+reader$>$}.
\end{DoxyItemize}

\subsection*{Using software}

\subsubsection*{\+\_\+bpbd-\/server\+\_\+ -\/ translation server}

{\itshape To\+Do\+: server console}

{\itshape To\+Do\+: Configuration file} \subsubsection*{\+\_\+bpbd-\/client\+\_\+ -\/ translation client}

\subsubsection*{\+\_\+lm-\/query\+\_\+ -\/ language model query tool}

In order to get the program usage information please run $\ast$./lm-\/query$\ast$ from the command line, the output of the program is supposed to be as follows\+:


\begin{DoxyCode}
1 vpn-stud-146-50-150-5:build zapreevis$ lm-query 
2 <...>
3 PARSE ERROR:  
4              Required arguments missing: query, model
5 
6 Brief USAGE: 
7    lm-query  [-l <lm lambda weight>] [-d <error|warn|usage|result|info
8              |info1|info2|info3>] -q <query file name> -m <model file name>
9              [--] [--version] [-h]
10 
11 For complete USAGE and HELP type: 
12    lm-query --help
\end{DoxyCode}


\subsection*{Input file formats}

\subsubsection*{Translatin model}

{\itshape To\+Do\+: Extend}

\subsubsection*{Reordering model}

{\itshape To\+Do\+: Extend}

\subsubsection*{Language model}

For machine translation it is important to estimate and compare the fluency of different possible translation outputs for the same source (i.\+e., foreign) sentence. This is commonly achieved by using a language model, which measures the probability of a string (which is commonly a sentence). Since entire sentences are unlikely to occur more than once, this is often approximated by using sliding windows of words (n-\/grams) occurring in some training data.

\paragraph*{Language Models background}

An {\itshape n-\/gram} refers to a continuous sequence of n tokens. For instance, given the following sentence\+: {\ttfamily our neighbor , who moved in recently , came by .} If n = 3, then the possible n-\/grams of this sentence include\+:


\begin{DoxyCode}
1 "our neighbor ,"
2 "neighbor , who"
3 ", who moved"
4 ...
5 ", came by"
6 "came by ."
\end{DoxyCode}
 Note that punctuation marks such as comma and full stop are treated just like any {\itshape real} word and that all words are lower cased.

\subsection*{Code documentation}

{\itshape To\+Do\+: Extend with more details}

At present the documentation is done in the Java-\/\+Doc style that is successfully accepted by Doxygen with the Doxygen option {\itshape J\+A\+V\+A\+D\+O\+C\+\_\+\+A\+U\+T\+O\+B\+R\+I\+E\+F} set to {\itshape Y\+E\+S}. The generated documentation is located in the $\ast$$\ast$./docs/$\ast$$\ast$ folder of the project.

\subsection*{External libraries}

{\itshape To\+Do\+: Write this section}

\subsection*{Performance evaluation}

In this section we provide an empirical comparison of the developed L\+M query tool with two other well known tools, namely \href{http://www.speech.sri.com/projects/srilm/}{\tt S\+R\+I\+L\+M} and \href{https://kheafield.com/code/kenlm/}{\tt Ken\+L\+M}, both of which provide language model implementations that can be queried. The additional information on the compared tools is to be found in \href{#appendix-tests}{\tt Appendix Tests}

\subsubsection*{Test set-\/up}

The main target of this experimental comparison is to evaluate memory consumption and query times of the implemented tries. For doing that we do not rely on the time and memory statis-\/ tics reported by the tools but rather, for the sake of uniform and independent opinion, rely on the Linux standard time utility available in the {\ttfamily zsh} Linux shell. The latter provides system-\/ measured statistics about the program run. We choose to measure\+:


\begin{DoxyItemize}
\item {\bfseries M\+R\+S\+S} -\/ the maximum resident memory usage of the program
\item {\bfseries C\+P\+U time} -\/ the C\+P\+U time in seconds
\end{DoxyItemize}

We chose to measure maximum resident memory usage as this is what defines the amount of R\+A\+M needed to run the program. Also, the C\+P\+U times are the actual times that the program was executed on the C\+P\+U. Measuring C\+P\+U times allows for a fair comparison as excludes possible results influence by the other system processes.

The experiments were set up to be run with different-\/size 5-\/gram language models given in the A\+R\+P\+A format with two types of inputs\+:


\begin{DoxyEnumerate}
\item The single 5-\/gram query that defines the baseline
\item The file input with 100,000,000 of 5-\/gram queries
\end{DoxyEnumerate}

The delta in execution C\+P\+U times between the baseline and the 100,000,000 query files defines the pure query execution time of the tool. Note that, the query files were produced from the text corpus different from the one used to produce the considered language models. The M\+R\+S\+S values are reported in gigabytes (Gb) and the C\+P\+U times are measured in seconds. The plots provide M\+R\+S\+S and C\+P\+U times relative to the input model size in Gb.

The test hardware configuration and the model/query files\textquotesingle{} data is to be found in \href{#appendix-tests}{\tt Appendix Tests}

\subsubsection*{Experimental results}

The experimental results are present in the following two pictures. The first one indicates the changes in the M\+R\+S\+S depending on the model size\+:



The second one shows the query C\+P\+U times depending on the model sizes\+:



The results show that the developed L\+M model trie representations are highly compatible with the available state of the art tools. We also give the following usage guidelines for the implemented tries\+:


\begin{DoxyItemize}
\item {\bfseries w2ca} and {\bfseries c2wa} tries are beneficial for the machines with limited R\+A\+M. If low memory usage is very critical then bitmap hash caching can also be disabled.
\item {\bfseries c2dm} trie provides the fastest performance with moderate memory consumption. This is recommended when high performance is needed but one should be aware of possible m-\/gram id collisions.\+10
\item {\bfseries c2dh} trie is preferable if performance, as well as moderate memory consumption, is needed. This is the second-\/fastest trie which, unlike {\bfseries c2dm}, is fully reliable.
\item {\bfseries w2ch} trie did not show itself useful and {\bfseries g2dm} is yet to be re-\/worked and improved for better performance and memory usage.
\item {\bfseries h2dm} following the intuitions of the Ken\+L\+M implementation, realises the hash-\/map based trie using the linear probing hash map which turns to be the fastest trie with one of the best memory consumption. This tries type is used as a default one
\end{DoxyItemize}

\subsection*{General design}

{\itshape To\+Do\+: Add the general design, the current and the future one withg images}

\subsection*{Software details}

\subsubsection*{\+\_\+bpbd-\/client\+\_\+}

{\itshape To\+Do\+: Add details on how the client works including requirements and structure} \subsubsection*{\+\_\+bpbd-\/server\+\_\+}

{\itshape To\+Do\+: Add details on how the server works including requirements and structure} \subsubsection*{\+\_\+lm-\/query\+\_\+}

{\itshape To\+Do\+: Update details on how the query tool works including requirements and structure}

\subsection*{Literature and references}

This project is originally based on the followin literature\+:

{\itshape To\+Do\+: Put the Bib\+Text entries into linked files} {\itshape To\+Do\+: Add the paper of Ken L\+M} {\itshape To\+Do\+: Add the S\+M\+T book}

The first paper discusses optimal Trie structures for storing the learned text corpus and the second indicates that using {\itshape std\+::unordered\+\_\+map} of C++ delivers one of the best time and space performances, compared to other data structures, when using for Trie implementations

{\itshape To\+Do\+: Add more details about the papers and books}

\subsection*{Licensing}

This is a free software\+: you can redistribute it and/or modify it under the terms of the G\+N\+U General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This software is distributed in the hope that it will be useful, but W\+I\+T\+H\+O\+U\+T A\+N\+Y W\+A\+R\+R\+A\+N\+T\+Y; without even the implied warranty of M\+E\+R\+C\+H\+A\+N\+T\+A\+B\+I\+L\+I\+T\+Y or F\+I\+T\+N\+E\+S\+S F\+O\+R A P\+A\+R\+T\+I\+C\+U\+L\+A\+R P\+U\+R\+P\+O\+S\+E. See the G\+N\+U General Public License for more details. You should have received a copy of the G\+N\+U General Public License along with this program. If not, see \href{http://www.gnu.org/licenses/}{\tt http\+://www.\+gnu.\+org/licenses/}.

\subsection*{History}


\begin{DoxyItemize}
\item {\bfseries 21.\+04.\+2015} -\/ Created
\item {\bfseries 27.\+07.\+2015} -\/ Changed project name and some to-\/do\textquotesingle{}s
\item {\bfseries 21.\+09.\+2015} -\/ Updated with the latest developments preparing for the version 1, Owl release.
\item {\bfseries 11.\+03.\+2016} -\/ Updated Updated to reflect the project status.
\end{DoxyItemize}

\subsection*{Appendix Tests}

\subsubsection*{S\+R\+I\+L\+M}

Is a toolkit for building and applying statistical language models (L\+Ms), primarily for use in speech recognition, statistical tagging and segmentation, and machine translation. It has been under development in the S\+R\+I Speech Technology and Research Laboratory since 1995. The employed tool version is {\bfseries 1.\+7.\+0}. The tool is run with the following command-\/line options\+: 
\begin{DoxyCode}
1 % ngram -lm model-file -order 5 -ppl queries-file \(\backslash\)
2       -no-sos -no-eos -memuse -debug 0
\end{DoxyCode}
 No changes were done to the tool’s source code.

\subsubsection*{Ken\+L\+M}

Ken\+L\+M is a tool for estimating, filtering, and querying language models. The tool does not have clear version indication, so we used the tool’s Git\+Hub snapshot of the Git revision\+:

{\itshape 0f 306088c3d8b3a668c934f 605e21b693b959d4d}

Ken\+L\+M does not allow to switch off the probability reports from the command line. Therefore we had to modify the tool’s code. In the {\ttfamily kenlm/lm/ngram query.\+hh} file we commented out the output code lines as follows\+:


\begin{DoxyCode}
1 struct BasicPrint \{
2   void Word(StringPiece, WordIndex, const FullScoreReturn &) const \{\}
3   void Line(uint64\_t oov, float total) const \{
4     //std::cout << "Total: " << total << " OOV: " << oov << ’\(\backslash\)n’;
5   \}
6   void Summary(double, double, uint64\_t, uint64\_t) \{\}
7 \};
8 struct FullPrint : public BasicPrint \{
9   void Word(StringPiece surface, WordIndex vocab,
10             const FullScoreReturn &ret) const \{
11     //std::cout << surface << ’=’ << vocab << ’ ’
12     //<< static\_cast<unsigned int>(ret.ngram\_length)
13     //<< ’ ’ << ret.prob << ’\(\backslash\)t’;
14 \}
15   void Summary(double ppl\_including\_oov, double ppl\_excluding\_oov,
16                uint64\_t corpus\_oov, uint64\_t corpus\_tokens) \{
17     std::cout <<
18       "Perplexity including OOVs:\(\backslash\)t" << ppl\_including\_oov << "\(\backslash\)n"
19       "Perplexity excluding OOVs:\(\backslash\)t" << ppl\_excluding\_oov << "\(\backslash\)n"
20       "OOVs:\(\backslash\)t" << corpus\_oov << "\(\backslash\)n"
21       "Tokens:\(\backslash\)t" << corpus\_tokens << ’\(\backslash\)n’
22       ;
23 \} \};
\end{DoxyCode}
 After this change, the tool was run with the following command-\/line options\+: 18 
\begin{DoxyCode}
1 % query -n model-file < queries-file
\end{DoxyCode}


\subsubsection*{Hardware configuration}

The experiments were run on the following machine configuration\+:


\begin{DoxyCode}
1 [~ smt7 ~]$ lscpu
2 Architecture:          x86\_64
3 CPU op-mode(s):        32-bit, 64-bit
4 Byte Order:            Little Endian
5 CPU(s):                40
6 On-line CPU(s) list:   0-39
7 Thread(s) per core:    2
8 Core(s) per socket:    10
9 Socket(s):             2
10 NUMA node(s):          2
11 Vendor ID:             GenuineIntel
12 CPU family:            6
13 Model:                 62
14 Stepping:              4
15 CPU MHz:               1200.000
16 BogoMIPS:              4999.23
17 Virtualization:        VT-x
18 L1d cache:             32K
19 L1i cache:             32K
20 L2 cache:              256K
21 L3 cache:              25600K
22 NUMA node0 CPU(s):     0-9,20-29
23 NUMA node1 CPU(s):     10-19,30-39
24 [~ smt7 ~]$ lsb\_release -irc
25 Distributor ID: CentOS
26 Release:    6.7
27 Codename:   Final
28 [~ smt7 ~]$ grep MemTotal /proc/meminfo
29 MemTotal:       264496688 kB
\end{DoxyCode}


\subsubsection*{Language models and query files}

The considered language models and their sizes (in bytes) are\+:


\begin{DoxyCode}
1 [~ smt10~]$ ls -al *.lm
2 -rw-r--r-- 1     937792965 Sep 21 15:55 e\_10\_641093.lm
3 -rw-r--r-- 1    1708763123 Sep 21 17:36 e\_20\_1282186.lm
4 -rw-r--r-- 1    3148711562 Sep 21 17:45 e\_30\_2564372.lm
5 -rw-r--r-- 1    5880154140 Sep 21 18:09 e\_40\_5128745.lm
6 -rw-r--r-- 1   10952178505 Sep 21 18:29 e\_50\_10257490.lm
7 -rw-r--r-- 1   15667577793 Sep 21 20:22 e\_60\_15386235.lm
8 -rw-r--r-- 1   20098725535 Sep 21 20:37 e\_70\_20514981.lm
9 -rw-r--r-- 1   48998103628 Sep 21 21:08 e\_80\_48998103628.lm
\end{DoxyCode}


The considered query files and their sizes are\+:


\begin{DoxyCode}
1 [~ smt10 ~]$ ls -al q\_5\_gram\_1*.txt
2 -rw-r--r-- 1   2697064872 Sep 21 15:47 q\_5\_gram\_100.000.000.txt
3 -rw-r--r-- 1           35 Sep 21 15:57 q\_5\_gram\_1.txt
4 [~ smt10 ~]$ 
\end{DoxyCode}


The number of m-\/grams per model is\+:

\subparagraph*{e\+\_\+10\+\_\+641093.\+lm}


\begin{DoxyCode}
1 [~ smt10 ~]$ head -n 15 e\_10\_641093.lm
2 \(\backslash\)data\(\backslash\)
3 ngram 1=105682
4 ngram 2=1737132
5 ngram 3=5121040
6 ngram 4=7659442
7 ngram 5=8741158
\end{DoxyCode}


\subparagraph*{e\+\_\+20\+\_\+1282186.\+lm}


\begin{DoxyCode}
1 [~ smt10 ~]$ head -n 8 e\_20\_1282186.lm
2 \(\backslash\)data\(\backslash\)
3 ngram 1=143867
4 ngram 2=2707890
5 ngram 3=8886067
6 ngram 4=14188078
7 ngram 5=16757214
\end{DoxyCode}


\#\#\#\#\#e\+\_\+30\+\_\+2564372.\+lm 
\begin{DoxyCode}
1 [~ smt10 ~]$ head -n 8 e\_30\_2564372.lm
2 \(\backslash\)data\(\backslash\)
3 ngram 1=199164
4 ngram 2=4202658
5 ngram 3=15300577
6 ngram 4=26097321
7 ngram 5=31952150
\end{DoxyCode}


\subparagraph*{e\+\_\+40\+\_\+5128745.\+lm}


\begin{DoxyCode}
1 [~ smt10 ~]$ head -n 8 e\_40\_5128745.lm
2 \(\backslash\)data\(\backslash\)
3 ngram 1=298070
4 ngram 2=6675818
5 ngram 3=26819467
6 ngram 4=48897704
7 ngram 5=62194729
\end{DoxyCode}


\subparagraph*{e\+\_\+50\+\_\+10257490.\+lm}


\begin{DoxyCode}
1 [~ smt10 ~]$ head -n 8 e\_50\_10257490.lm
2 \(\backslash\)data\(\backslash\)
3 ngram 1=439499
4 ngram 2=10447874
5 ngram 3=46336705
6 ngram 4=90709359
7 ngram 5=120411272
\end{DoxyCode}


\subparagraph*{e\+\_\+60\+\_\+15386235.\+lm}


\begin{DoxyCode}
1 [~ smt10 ~]$ head -n 8 e\_60\_15386235.lm
2 \(\backslash\)data\(\backslash\)
3 ngram 1=568105
4 ngram 2=13574606
5 ngram 3=63474074
6 ngram 4=129430409
7 ngram 5=176283104
\end{DoxyCode}


\subparagraph*{e\+\_\+70\+\_\+20514981.\+lm}


\begin{DoxyCode}
1 [~ smt10 ~]$ head -n 8 e\_70\_20514981.lm
2 \(\backslash\)data\(\backslash\)
3 ngram 1=676750
4 ngram 2=16221298
5 ngram 3=78807519
6 ngram 4=165569280
7 ngram 5=229897626
\end{DoxyCode}


\subparagraph*{e\+\_\+80\+\_\+48998103628.\+lm}


\begin{DoxyCode}
1 [~ smt10 ~]$ head -n 8 e\_80\_48998103628.lm
2 \(\backslash\)data\(\backslash\)
3 ngram 1=2210728
4 ngram 2=67285057
5 ngram 3=183285165
6 ngram 4=396600722
7 ngram 5=563533665
\end{DoxyCode}




{\itshape Powered by \href{https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet}{\tt Markdown-\/\+Cheatsheet}} 